{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackathon\n",
    "\n",
    "Introduction to Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenAI key from env\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "azure_api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "Missing credentials. Please pass one of `api_key`, `azure_ad_token`, `azure_ad_token_provider`, or the `AZURE_OPENAI_API_KEY` or `AZURE_OPENAI_AD_TOKEN` environment variables.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/kris/projects/Pub Quiz vs GPT /Hackathon-Pubquiz/hackathon.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kris/projects/Pub%20Quiz%20vs%20GPT%20/Hackathon-Pubquiz/hackathon.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# create llm instance\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kris/projects/Pub%20Quiz%20vs%20GPT%20/Hackathon-Pubquiz/hackathon.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchat_models\u001b[39;00m \u001b[39mimport\u001b[39;00m AzureChatOpenAI\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kris/projects/Pub%20Quiz%20vs%20GPT%20/Hackathon-Pubquiz/hackathon.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m llm \u001b[39m=\u001b[39m AzureChatOpenAI(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kris/projects/Pub%20Quiz%20vs%20GPT%20/Hackathon-Pubquiz/hackathon.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     api_key\u001b[39m=\u001b[39;49mazure_api_key,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kris/projects/Pub%20Quiz%20vs%20GPT%20/Hackathon-Pubquiz/hackathon.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     api_version\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m2023-05-15\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kris/projects/Pub%20Quiz%20vs%20GPT%20/Hackathon-Pubquiz/hackathon.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     azure_deployment\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-35-turbo-16k\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kris/projects/Pub%20Quiz%20vs%20GPT%20/Hackathon-Pubquiz/hackathon.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     azure_endpoint\u001b[39m=\u001b[39;49mazure_endpoint,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kris/projects/Pub%20Quiz%20vs%20GPT%20/Hackathon-Pubquiz/hackathon.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:171\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     warned \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     emit_warning()\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:171\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m     warned \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     emit_warning()\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain_core/load/serializable.py:107\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    108\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lc_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pydantic/v1/main.py:339\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39mCreate a new model by parsing and validating input data from keyword arguments.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[39mRaises ValidationError if the input data cannot be parsed to form a valid model.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[39m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m values, fields_set, validation_error \u001b[39m=\u001b[39m validate_model(__pydantic_self__\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m, data)\n\u001b[1;32m    340\u001b[0m \u001b[39mif\u001b[39;00m validation_error:\n\u001b[1;32m    341\u001b[0m     \u001b[39mraise\u001b[39;00m validation_error\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pydantic/v1/main.py:1102\u001b[0m, in \u001b[0;36mvalidate_model\u001b[0;34m(model, input_data, cls)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1102\u001b[0m     values \u001b[39m=\u001b[39m validator(cls_, values)\n\u001b[1;32m   1103\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m, \u001b[39mAssertionError\u001b[39;00m) \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m   1104\u001b[0m     errors\u001b[39m.\u001b[39mappend(ErrorWrapper(exc, loc\u001b[39m=\u001b[39mROOT_KEY))\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain_community/chat_models/azure_openai.py:209\u001b[0m, in \u001b[0;36mAzureChatOpenAI.validate_environment\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    193\u001b[0m             values[\u001b[39m\"\u001b[39m\u001b[39mdeployment_name\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     client_params \u001b[39m=\u001b[39m {\n\u001b[1;32m    195\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mapi_version\u001b[39m\u001b[39m\"\u001b[39m: values[\u001b[39m\"\u001b[39m\u001b[39mopenai_api_version\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    196\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mazure_endpoint\u001b[39m\u001b[39m\"\u001b[39m: values[\u001b[39m\"\u001b[39m\u001b[39mazure_endpoint\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttp_client\u001b[39m\u001b[39m\"\u001b[39m: values[\u001b[39m\"\u001b[39m\u001b[39mhttp_client\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    208\u001b[0m     }\n\u001b[0;32m--> 209\u001b[0m     values[\u001b[39m\"\u001b[39m\u001b[39mclient\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mAzureOpenAI(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mclient_params)\u001b[39m.\u001b[39mchat\u001b[39m.\u001b[39mcompletions\n\u001b[1;32m    210\u001b[0m     values[\u001b[39m\"\u001b[39m\u001b[39masync_client\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mAsyncAzureOpenAI(\n\u001b[1;32m    211\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mclient_params\n\u001b[1;32m    212\u001b[0m     )\u001b[39m.\u001b[39mchat\u001b[39m.\u001b[39mcompletions\n\u001b[1;32m    213\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/lib/azure.py:166\u001b[0m, in \u001b[0;36mAzureOpenAI.__init__\u001b[0;34m(self, api_version, azure_endpoint, azure_deployment, api_key, azure_ad_token, azure_ad_token_provider, organization, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    163\u001b[0m     azure_ad_token \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mAZURE_OPENAI_AD_TOKEN\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m api_key \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m azure_ad_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m azure_ad_token_provider \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    167\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMissing credentials. Please pass one of `api_key`, `azure_ad_token`, `azure_ad_token_provider`, or the `AZURE_OPENAI_API_KEY` or `AZURE_OPENAI_AD_TOKEN` environment variables.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    170\u001b[0m \u001b[39mif\u001b[39;00m api_version \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m     api_version \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mOPENAI_API_VERSION\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mOpenAIError\u001b[0m: Missing credentials. Please pass one of `api_key`, `azure_ad_token`, `azure_ad_token_provider`, or the `AZURE_OPENAI_API_KEY` or `AZURE_OPENAI_AD_TOKEN` environment variables."
     ]
    }
   ],
   "source": [
    "# create llm instance\n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    api_key=azure_api_key,\n",
    "    api_version=\"2023-05-15\",\n",
    "    azure_deployment=\"gpt-35-turbo-16k\",\n",
    "    azure_endpoint=azure_endpoint,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke llm\n",
    "\n",
    "llm.invoke(\"What do you know about Pub Quizzes?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use prompts\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"You are an encyclopedia.\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"input\": \"What are Pub Quizzes?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reusue prompts\n",
    "\n",
    "chain.invoke({\"input\": \"What are Pub Quizzes also called?\"})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contexts\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"input\": \"What are Pub Quizzes also called?\",\n",
    "        \"context\": \"A pub quiz is a quiz held in a pub or bar. These events are also called quiz nights, trivia nights, or bar trivia and may be held in other settings. The pub quiz is a modern example of a pub game, and often attempts to lure customers to the establishment on quieter days. The pub quiz has become part of British culture since its popularization in the UK in the 1970s by Burns and Porter, although the first mentions in print can be traced to 1959.[4][5] It then became a staple in Irish pub culture, and its popularity has continued to spread internationally. Although different pub quizzes can cover a range of formats and topics, they have many features in common. Most quizzes have a limited number of team members, offer prizes for winning teams, and distinguish rounds by category or theme. \",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"A pub quiz is a quiz held in a pub or bar. These events are also called quiz nights, trivia nights, or bar trivia and may be held in other settings. The pub quiz is a modern example of a pub game, and often attempts to lure customers to the establishment on quieter days. The pub quiz has become part of British culture since its popularization in the UK in the 1970s by Burns and Porter, although the first mentions in print can be traced to 1959.[4][5] It then became a staple in Irish pub culture, and its popularity has continued to spread internationally. Although different pub quizzes can cover a range of formats and topics, they have many features in common. Most quizzes have a limited number of team members, offer prizes for winning teams, and distinguish rounds by category or theme. \",\n",
    "        metadata={\n",
    "            \"source\": \"wikipedia\"\n",
    "        }\n",
    "    )\n",
    "]\n",
    "\n",
    "document_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "document_chain.invoke(\n",
    "    {\n",
    "        \"input\": \"What are Pub Quizzes also called?\",\n",
    "        \"context\": documents,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chain.invoke(\n",
    "    {\n",
    "        \"input\": \"What are Pub Quizzes also called and what is the source?\",\n",
    "        \"context\": documents,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document prompts\n",
    "\n",
    "document_prompt = ChatPromptTemplate.from_template(\"\"\"Content: {page_content}                             \n",
    "Source: {source}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    document_prompt=document_prompt,\n",
    ")\n",
    "\n",
    "document_chain.invoke(\n",
    "    {\n",
    "        \"input\": \"What are Pub Quizzes also called and what is the source?\",\n",
    "        \"context\": documents,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector stores\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.azure_openai import AzureOpenAIEmbeddings\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "\n",
    "\n",
    "# load data\n",
    "loader = TextLoader(r\"./data/text/pubquiz.txt\", encoding=\"utf-8\")\n",
    "data = loader.load()\n",
    "loader = TextLoader(r\"./data/text/llm.txt\", encoding=\"utf-8\")\n",
    "data.extend(loader.load())\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20, separators=[\".\", \"\\n\"])\n",
    "documents = splitter.split_documents(data)\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    api_key=azure_api_key,\n",
    "    api_version=\"2023-05-15\",\n",
    "    azure_deployment=\"text-embedding-ada-002\",\n",
    "    azure_endpoint=azure_endpoint,\n",
    ")\n",
    "db = Chroma.from_documents(documents, embeddings, persist_directory=\"./chroma/quiz\")\n",
    "\n",
    "for document in documents:\n",
    "    print(document)\n",
    "    print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "db = Chroma(persist_directory=\"./chroma/quiz\", embedding_function=embeddings)\n",
    "retriever = db.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "retrieval_chain.invoke({\"input\": \"What are Pub Quizzes also called and what is the source?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain.invoke({\"input\": \"Give a concise description of a LLM and what is the source?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add already embedded data\n",
    "\n",
    "db_book = Chroma(persist_directory=\"./chroma/book\", embedding_function=embeddings)\n",
    "db_book_data = db_book._collection.get(include=['documents', 'metadatas', 'embeddings'])\n",
    "\n",
    "db._collection.add(\n",
    "    embeddings=db_book_data['embeddings'],\n",
    "    metadatas=db_book_data['metadatas'],\n",
    "    documents=db_book_data['documents'],\n",
    "    ids=db_book_data['ids']\n",
    ")\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain.invoke({\"input\": \"What does Juliet answer when Paris says 'So will ye, I am sure, that you love me.'?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_prompt = ChatPromptTemplate.from_template(\"\"\"Please summarize the following piece of text.\n",
    "Respond in a manner that a 5 year old would understand.\n",
    "\n",
    "Text: {context}\"\"\")\n",
    "\n",
    "summary_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=summary_prompt,\n",
    "    document_prompt=document_prompt,\n",
    ")\n",
    "\n",
    "summary_chain.invoke(\n",
    "    {\"context\": documents[:3]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context size\n",
    "\n",
    "summary_chain.invoke(\n",
    "    {\"context\": documents}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from langchain.chains.combine_documents import collapse_docs, split_list_of_docs\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.prompts import format_document\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "partial_format_document = partial(format_document, prompt=document_prompt)\n",
    "\n",
    "# The chain we'll apply to each individual document.\n",
    "# Returns a summary of the document.\n",
    "map_chain = (\n",
    "    {\"context\": partial_format_document}\n",
    "    | summary_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# A wrapper chain to keep the original Document metadata\n",
    "map_as_doc_chain = (\n",
    "    RunnableParallel({\"doc\": RunnablePassthrough(), \"content\": map_chain})\n",
    "    | (lambda x: Document(page_content=x[\"content\"], metadata=x[\"doc\"].metadata))\n",
    ").with_config(run_name=\"Summarize (return doc)\")\n",
    "\n",
    "# The chain we'll repeatedly apply to collapse subsets of the documents\n",
    "# into a consolidate document until the total token size of our\n",
    "# documents is below some max size.\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(partial_format_document(doc) for doc in docs)\n",
    "\n",
    "collapse_chain = (\n",
    "    {\"context\": format_docs}\n",
    "    | PromptTemplate.from_template(\"Collapse this content:\\n\\n{context}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "def get_num_tokens(docs):\n",
    "    return llm.get_num_tokens(format_docs(docs))\n",
    "\n",
    "def collapse(\n",
    "    docs,\n",
    "    config,\n",
    "    token_max=4000,\n",
    "):\n",
    "    collapse_ct = 1\n",
    "    while get_num_tokens(docs) > token_max:\n",
    "        config[\"run_name\"] = f\"Collapse {collapse_ct}\"\n",
    "        invoke = partial(collapse_chain.invoke, config=config)\n",
    "        split_docs = split_list_of_docs(docs, get_num_tokens, token_max)\n",
    "        docs = [collapse_docs(_docs, invoke) for _docs in split_docs]\n",
    "        collapse_ct += 1\n",
    "    return docs\n",
    "\n",
    "# The chain we'll use to combine our individual document summaries\n",
    "# (or summaries over subset of documents if we had to collapse the map results)\n",
    "# into a final summary.\n",
    "\n",
    "reduce_chain = (\n",
    "    {\"context\": format_docs}\n",
    "    | PromptTemplate.from_template(\"Combine these summaries:\\n\\n{context}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ").with_config(run_name=\"Reduce\")\n",
    "\n",
    "# The final full chain\n",
    "map_reduce = (map_as_doc_chain.map() | collapse | reduce_chain).with_config(run_name=\"Map reduce\")\n",
    "\n",
    "map_reduce.invoke(\n",
    "    input=documents,\n",
    "    config={\"max_concurrency\": 5},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map reduce takes longer\n",
    "\n",
    "map_reduce.invoke(\n",
    "    input=documents[:3],\n",
    "    config={\"max_concurrency\": 5},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few shotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"A question can have an intent of either Document Retrieval, Scientific Question or Political Question.\n",
    "Now tell me which intent the following question has.\n",
    "                                            \n",
    "Question: {input}\"\"\")\n",
    "\n",
    "intent_chain = intent_prompt | llm\n",
    "intent_chain.invoke({\"input\": \"How fast can a Document fall if dropped?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"A question can have an intent of either Document Retrieval, Scientific Question or Political Question.\n",
    "Now tell me which intent the following question has. Only answer with two words.\n",
    "                                            \n",
    "Question: {input}\"\"\")\n",
    "\n",
    "intent_chain = intent_prompt | llm\n",
    "intent_chain.invoke({\"input\": \"How fast can a Document fall if dropped?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"A question can have an intent of either Document Retrieval, Scientific Question or Political Question.\n",
    "\n",
    "Consider the following examples:\n",
    "\n",
    "Question: What are the Panama Papers?\n",
    "Answer: Document Retrieval\n",
    "\n",
    "Question: What is the maximum velocity of a book when thrown?\n",
    "Answer: Scientific Question\n",
    "\n",
    "Now tell me which intent the follolwing question has. Only answer with two words.\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "intent_chain = intent_prompt | llm\n",
    "intent_chain.invoke({\"input\": \"How fast can a Document fall if dropped?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent and Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"What did Frank-Walter Steinmeier say in his christmas speech 2023?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "from langchain.tools.ddg_search import DuckDuckGoSearchRun\n",
    "\n",
    "ddg = DuckDuckGoSearchRun()\n",
    "\n",
    "ddg_tool = Tool.from_function(\n",
    "    func = ddg.run,\n",
    "    name = \"DuckDuckGo Search\",\n",
    "    description = \"Search DuckDuckGo for a query abount current events.\",\n",
    ")\n",
    "\n",
    "tools = [ddg_tool]\n",
    "\n",
    "ddg_tool.run(\"What did Frank-Walter Steinmeier say in his christmas speech 2023?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    ")\n",
    "\n",
    "agent.invoke({\"input\": \"What did Frank-Walter Steinmeier say in his christmas speech 2023?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREFIX = \"\"\"You are participating in a pubquiz. Answer in a short sentence.\"\"\"\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    "    agent_kwargs={\n",
    "        'prefix': PREFIX\n",
    "    }\n",
    ")\n",
    "\n",
    "agent.invoke({\"input\": \"What did Frank-Walter Steinmeier say in his christmas speech 2023?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_summary_prompt = ChatPromptTemplate.from_template(\"\"\"Please summarize the following piece of text.\n",
    "Respond in a manner that a 5 year old would understand.\n",
    "\n",
    "Text: {input}\"\"\")\n",
    "simple_summary_chain = {\"input\": RunnablePassthrough()} | simple_summary_prompt | llm\n",
    "\n",
    "summary_tool = Tool(\n",
    "    name=\"Summary Tool\",\n",
    "    func=simple_summary_chain.invoke,\n",
    "    description=\"Use this tool to do a summary. Make sure you get the text to do a summary of first.\"\n",
    ")\n",
    "tools.append(summary_tool)\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    ")\n",
    "\n",
    "agent.invoke({\"input\": \"Summarize the christmas speech 2023 of Frank-Walter Steinmeier?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"What is 1000 + 1234?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"What is 13 raised to the .3432 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m azure_api_key_whisper \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAZURE_OPENAI_API_KEY_WHISPER\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m azure_endpoint_whisper \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAZURE_OPENAI_ENDPOINT_WHISPER\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m client \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mAzureOpenAI(\n\u001b[0;32m      7\u001b[0m     api_key\u001b[38;5;241m=\u001b[39mazure_api_key_whisper,\n\u001b[0;32m      8\u001b[0m     azure_endpoint\u001b[38;5;241m=\u001b[39mazure_endpoint_whisper,\n\u001b[0;32m      9\u001b[0m     azure_deployment\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhisper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     api_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2023-09-01-preview\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "azure_api_key_whisper = os.getenv('AZURE_OPENAI_API_KEY_WHISPER')\n",
    "azure_endpoint_whisper = os.getenv('AZURE_OPENAI_ENDPOINT_WHISPER')\n",
    "\n",
    "client = openai.AzureOpenAI(\n",
    "    api_key=azure_api_key_whisper,\n",
    "    azure_endpoint=azure_endpoint_whisper,\n",
    "    azure_deployment=\"whisper\",\n",
    "    api_version=\"2023-09-01-preview\",\n",
    ")\n",
    "\n",
    "def get_transcript(audio_file):\n",
    "    if not os.path.exists(audio_file):\n",
    "        audio_file = \"./data/\" + audio_file\n",
    "    client.audio.with_raw_response\n",
    "    return client.audio.transcriptions.create(\n",
    "        file=open(audio_file, \"rb\"),            \n",
    "        model=\"whisper\",\n",
    "        language=\"de\",\n",
    "    ).text\n",
    "\n",
    "audio_test_file = \"./data/audio/newyear2023.mp3\"\n",
    "get_transcript(audio_test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio tool\n",
    "\n",
    "def available_audio_files(input):\n",
    "    return \"./data/audio/newyear2023.mp3 - new years eve speech 2023 of Olaf Scholz\\n\"\\\n",
    "        \"./data/audio/newyear2016.mp3 - new years eve speech 2016 of Angela Merkel\\n\" \\\n",
    "        \"./data/audio/christmas2019.mp3 - christmas speech 2019 of Frank-Walter Steinmeyer\\n\"\n",
    "\n",
    "audio_file_tool = Tool(\n",
    "    name=\"Audio File Tool\",\n",
    "    func=available_audio_files,\n",
    "    description=\"Use this tool to see which new years eve speeches are available as an audio file.\"\n",
    ")\n",
    "\n",
    "# long summary tool\n",
    "\n",
    "def split_text_to_docs(text):\n",
    "    doc = Document(\n",
    "        page_content=text,\n",
    "        metadata={\n",
    "            \"source\": \"text\"\n",
    "        }\n",
    "    )\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=2500, chunk_overlap=20, separators=[\".\", \"\\n\"])\n",
    "    return splitter.split_documents([doc])\n",
    "\n",
    "def get_splitted_transcript(audio_file):\n",
    "    return split_text_to_docs(get_transcript(audio_file))\n",
    "\n",
    "long_summary_chain = get_splitted_transcript | map_reduce\n",
    "\n",
    "audio_tool = Tool(\n",
    "    name=\"Audio Tool\",\n",
    "    func=long_summary_chain.invoke,\n",
    "    description=\"Use this tool to get the summary of the new years speech in a given audio file.\"\n",
    ")\n",
    "\n",
    "agent = initialize_agent(\n",
    "    [\n",
    "        summary_tool,\n",
    "        audio_file_tool,\n",
    "        audio_tool,\n",
    "        ddg_tool\n",
    "    ],\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    ")\n",
    "\n",
    "agent.invoke({\"input\": \"Summarize the new years eve speech 2023 of Olaf Scholz using the audio and summary tool?\"})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
